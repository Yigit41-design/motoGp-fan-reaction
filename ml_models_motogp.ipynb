{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required libraries\n",
        "!pip install pandas numpy scikit-learn scipy matplotlib seaborn -q\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Data Loading and Preparation\n",
        "\n",
        "This cell loads MotoGP data or uses cached data if available.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Data directory\n",
        "DATA_DIR = Path(\"ml_data\")\n",
        "DATA_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "def load_motogp_data():\n",
        "    \"\"\"\n",
        "    Load MotoGP data or use cached data\n",
        "    \"\"\"\n",
        "    cache_file = DATA_DIR / \"motogp_full_cache.pkl\"\n",
        "    \n",
        "    # Load cached data if exists\n",
        "    if cache_file.exists():\n",
        "        print(\"‚úÖ Loading cached MotoGP data...\")\n",
        "        with open(cache_file, 'rb') as f:\n",
        "            data = pickle.load(f)\n",
        "        print(f\"   Data loaded: {len(data)} rows, {len(data.columns)} columns\")\n",
        "        print(f\"   Columns: {list(data.columns[:10])}...\")\n",
        "        return data\n",
        "    \n",
        "    # Load from CSV if exists\n",
        "    csv_file = Path(\"trends_data\") / \"motogp_full_data.csv\"\n",
        "    if csv_file.exists():\n",
        "        print(\"‚úÖ Loading MotoGP data from CSV...\")\n",
        "        data = pd.read_csv(csv_file)\n",
        "        if 'RaceDate' in data.columns:\n",
        "            data['RaceDate'] = pd.to_datetime(data['RaceDate'])\n",
        "        print(f\"   Data loaded: {len(data)} rows\")\n",
        "        # Save cache\n",
        "        with open(cache_file, 'wb') as f:\n",
        "            pickle.dump(data, f)\n",
        "        print(f\"   Data cached: {cache_file}\")\n",
        "        return data\n",
        "    \n",
        "    # If motogp_full variable exists in main notebook (Jupyter kernel)\n",
        "    try:\n",
        "        # If running in Jupyter/IPython environment, get variable from main notebook\n",
        "        import sys\n",
        "        if 'ipykernel' in sys.modules or 'IPython' in sys.modules:\n",
        "            # Get from IPython namespace\n",
        "            try:\n",
        "                from IPython import get_ipython\n",
        "                ipython = get_ipython()\n",
        "                if ipython is not None and 'motogp_full' in ipython.user_ns:\n",
        "                    print(\"‚úÖ Loading motogp_full from main notebook...\")\n",
        "                    data = ipython.user_ns['motogp_full'].copy()\n",
        "                    print(f\"   Data loaded: {len(data)} rows\")\n",
        "                    # Save cache\n",
        "                    with open(cache_file, 'wb') as f:\n",
        "                        pickle.dump(data, f)\n",
        "                    print(f\"   Data cached: {cache_file}\")\n",
        "                    return data\n",
        "            except:\n",
        "                pass\n",
        "    except:\n",
        "        pass\n",
        "    \n",
        "    print(\"‚ö†Ô∏è  No cached data found.\")\n",
        "    print(\"   Please run the main analysis notebook first to create motogp_full data.\")\n",
        "    print(\"   Or load the data manually in the cell below.\")\n",
        "    return None\n",
        "\n",
        "# Load data\n",
        "motogp_data = load_motogp_data()\n",
        "\n",
        "# Show preview if data loaded\n",
        "if motogp_data is not None:\n",
        "    print(f\"\\nüìä Data Preview:\")\n",
        "    print(f\"   Shape: {motogp_data.shape}\")\n",
        "    print(f\"   Columns ({len(motogp_data.columns)}):\")\n",
        "    for i, col in enumerate(motogp_data.columns):\n",
        "        print(f\"     {i+1}. {col}\")\n",
        "    print(f\"\\n   First 5 rows:\")\n",
        "    print(motogp_data.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Preparation and Feature Engineering\n",
        "\n",
        "Target variable: Classification based on **Relative Increase %** categories\n",
        "- **Low**: < 0% (decrease or no change)\n",
        "- **Medium**: 0-50% (moderate increase)\n",
        "- **High**: > 50% (high increase)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def prepare_ml_data(df):\n",
        "    \"\"\"\n",
        "    Prepare MotoGP data for ML\n",
        "    \n",
        "    Args:\n",
        "        df: motogp_full DataFrame\n",
        "    \n",
        "    Returns:\n",
        "        X: Feature matrix\n",
        "        y: Target labels\n",
        "        feature_names: Feature names\n",
        "    \"\"\"\n",
        "    # Copy data\n",
        "    data = df.copy()\n",
        "    \n",
        "    # Clean NaN values\n",
        "    data = data.dropna(subset=['Relative Increase %', 'Popularity', \n",
        "                               'Career wins_num', 'Career podiums_num', \n",
        "                               'Championships_num'])\n",
        "    \n",
        "    # Clean infinite values\n",
        "    data = data[np.isfinite(data['Relative Increase %'])]\n",
        "    \n",
        "    # Categorize target variable\n",
        "    def categorize_increase(increase):\n",
        "        if pd.isna(increase) or not np.isfinite(increase):\n",
        "            return None\n",
        "        if increase < 0:\n",
        "            return 'Low'\n",
        "        elif increase < 50:\n",
        "            return 'Medium'\n",
        "        else:\n",
        "            return 'High'\n",
        "    \n",
        "    data['target_category'] = data['Relative Increase %'].apply(categorize_increase)\n",
        "    data = data.dropna(subset=['target_category'])\n",
        "    \n",
        "    # Select features\n",
        "    feature_cols = [\n",
        "        'Popularity',\n",
        "        'Career wins_num',\n",
        "        'Career podiums_num',\n",
        "        'Championships_num',\n",
        "        'Years_active_len',\n",
        "        'Search Before',\n",
        "        'Trend Difference'\n",
        "    ]\n",
        "    \n",
        "    # Use only available columns\n",
        "    available_features = [col for col in feature_cols if col in data.columns]\n",
        "    \n",
        "    X = data[available_features].values\n",
        "    y = data['target_category'].values\n",
        "    \n",
        "    # Label encoding\n",
        "    le = LabelEncoder()\n",
        "    y_encoded = le.fit_transform(y)\n",
        "    \n",
        "    print(f\"‚úÖ Data prepared:\")\n",
        "    print(f\"   Number of samples: {len(X)}\")\n",
        "    print(f\"   Number of features: {X.shape[1]}\")\n",
        "    print(f\"   Features: {available_features}\")\n",
        "    print(f\"   Class distribution:\")\n",
        "    for label, count in zip(le.classes_, np.bincount(y_encoded)):\n",
        "        print(f\"     {label}: {count}\")\n",
        "    \n",
        "    return X, y_encoded, available_features, le\n",
        "\n",
        "# Prepare data (if motogp_data exists)\n",
        "if motogp_data is not None:\n",
        "    X, y, feature_names, label_encoder = prepare_ml_data(motogp_data)\n",
        "    \n",
        "    # Train/test split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=42, stratify=y\n",
        "    )\n",
        "    \n",
        "    # Scaling\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "    \n",
        "    print(f\"\\n‚úÖ Train/Test split completed:\")\n",
        "    print(f\"   Train: {X_train.shape[0]} samples\")\n",
        "    print(f\"   Test: {X_test.shape[0]} samples\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  Data not loaded. Please load the data first.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Random Forest Classifier\n",
        "\n",
        "Main classifier model - Predicts MotoGP fan reaction categories.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "def train_random_forest(X_train, X_test, y_train, y_test, \n",
        "                        n_estimators=100, max_depth=None, random_state=42):\n",
        "    \"\"\"\n",
        "    Train Random Forest model\n",
        "    \n",
        "    Args:\n",
        "        X_train, X_test: Training and test features\n",
        "        y_train, y_test: Training and test labels\n",
        "        n_estimators: Number of trees\n",
        "        max_depth: Maximum depth\n",
        "        random_state: Random seed\n",
        "    \n",
        "    Returns:\n",
        "        model: Trained model\n",
        "    \"\"\"\n",
        "    print(\"üå≤ Training Random Forest...\")\n",
        "    print(f\"   Parameters: n_estimators={n_estimators}, max_depth={max_depth}\")\n",
        "    \n",
        "    # Create model\n",
        "    model = RandomForestClassifier(\n",
        "        n_estimators=n_estimators,\n",
        "        max_depth=max_depth,\n",
        "        random_state=random_state,\n",
        "        n_jobs=-1,\n",
        "        class_weight='balanced'  # Handle class imbalance\n",
        "    )\n",
        "    \n",
        "    # Train\n",
        "    model.fit(X_train, y_train)\n",
        "    \n",
        "    # Predict\n",
        "    y_pred_train = model.predict(X_train)\n",
        "    y_pred_test = model.predict(X_test)\n",
        "    \n",
        "    # Evaluate\n",
        "    train_acc = accuracy_score(y_train, y_pred_train)\n",
        "    test_acc = accuracy_score(y_test, y_pred_test)\n",
        "    \n",
        "    print(f\"\\n‚úÖ Model trained!\")\n",
        "    print(f\"   Train Accuracy: {train_acc:.4f}\")\n",
        "    print(f\"   Test Accuracy: {test_acc:.4f}\")\n",
        "    \n",
        "    # Feature importances\n",
        "    feature_importance = pd.DataFrame({\n",
        "        'feature': feature_names,\n",
        "        'importance': model.feature_importances_\n",
        "    }).sort_values('importance', ascending=False)\n",
        "    \n",
        "    print(f\"\\nüìä Feature Importances:\")\n",
        "    print(feature_importance.to_string(index=False))\n",
        "    \n",
        "    # Classification report\n",
        "    print(f\"\\nüìä Classification Report (Test):\")\n",
        "    print(classification_report(y_test, y_pred_test, \n",
        "                                target_names=label_encoder.classes_))\n",
        "    \n",
        "    # Save model\n",
        "    model_file = DATA_DIR / \"random_forest_model.pkl\"\n",
        "    with open(model_file, 'wb') as f:\n",
        "        pickle.dump({\n",
        "            'model': model,\n",
        "            'scaler': scaler,\n",
        "            'label_encoder': label_encoder,\n",
        "            'feature_names': feature_names\n",
        "        }, f)\n",
        "    print(f\"\\nüíæ Model saved: {model_file}\")\n",
        "    \n",
        "    return model, feature_importance\n",
        "\n",
        "# Train model (if data is ready)\n",
        "if 'X_train_scaled' in locals() and 'X_test_scaled' in locals():\n",
        "    rf_model, feature_importance = train_random_forest(\n",
        "        X_train_scaled, X_test_scaled, y_train, y_test,\n",
        "        n_estimators=100, max_depth=10\n",
        "    )\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  Data not ready. Please run the data preparation cell first.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Decision Tree Classifier\n",
        "\n",
        "Baseline model - Used for explanation and comparison in the report.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def train_decision_tree(X_train, X_test, y_train, y_test,\n",
        "                       max_depth=5, min_samples_split=10, random_state=42):\n",
        "    \"\"\"\n",
        "    Train Decision Tree model (baseline)\n",
        "    \n",
        "    Args:\n",
        "        X_train, X_test: Training and test features\n",
        "        y_train, y_test: Training and test labels\n",
        "        max_depth: Maximum depth\n",
        "        min_samples_split: Minimum samples for split\n",
        "        random_state: Random seed\n",
        "    \n",
        "    Returns:\n",
        "        model: Trained model\n",
        "    \"\"\"\n",
        "    print(\"üå≥ Training Decision Tree...\")\n",
        "    print(f\"   Parameters: max_depth={max_depth}, min_samples_split={min_samples_split}\")\n",
        "    \n",
        "    # Create model\n",
        "    model = DecisionTreeClassifier(\n",
        "        max_depth=max_depth,\n",
        "        min_samples_split=min_samples_split,\n",
        "        random_state=random_state,\n",
        "        class_weight='balanced'\n",
        "    )\n",
        "    \n",
        "    # Train\n",
        "    model.fit(X_train, y_train)\n",
        "    \n",
        "    # Predict\n",
        "    y_pred_train = model.predict(X_train)\n",
        "    y_pred_test = model.predict(X_test)\n",
        "    \n",
        "    # Evaluate\n",
        "    train_acc = accuracy_score(y_train, y_pred_train)\n",
        "    test_acc = accuracy_score(y_test, y_pred_test)\n",
        "    \n",
        "    print(f\"\\n‚úÖ Model trained!\")\n",
        "    print(f\"   Train Accuracy: {train_acc:.4f}\")\n",
        "    print(f\"   Test Accuracy: {test_acc:.4f}\")\n",
        "    \n",
        "    # Classification report\n",
        "    print(f\"\\nüìä Classification Report (Test):\")\n",
        "    print(classification_report(y_test, y_pred_test,\n",
        "                                target_names=label_encoder.classes_))\n",
        "    \n",
        "    # Tree visualization (for small depth)\n",
        "    if max_depth <= 5:\n",
        "        plt.figure(figsize=(20, 10))\n",
        "        plot_tree(model, \n",
        "                  feature_names=feature_names,\n",
        "                  class_names=label_encoder.classes_,\n",
        "                  filled=True,\n",
        "                  rounded=True,\n",
        "                  fontsize=10)\n",
        "        plt.title(\"Decision Tree Visualization\", fontsize=16)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(DATA_DIR / \"decision_tree_visualization.png\", dpi=300, bbox_inches='tight')\n",
        "        print(f\"\\nüìä Tree visualization saved: {DATA_DIR / 'decision_tree_visualization.png'}\")\n",
        "        plt.show()\n",
        "    \n",
        "    # Save model\n",
        "    model_file = DATA_DIR / \"decision_tree_model.pkl\"\n",
        "    with open(model_file, 'wb') as f:\n",
        "        pickle.dump({\n",
        "            'model': model,\n",
        "            'scaler': scaler,\n",
        "            'label_encoder': label_encoder,\n",
        "            'feature_names': feature_names\n",
        "        }, f)\n",
        "    print(f\"\\nüíæ Model saved: {model_file}\")\n",
        "    \n",
        "    return model\n",
        "\n",
        "# Train model (if data is ready)\n",
        "if 'X_train_scaled' in locals() and 'X_test_scaled' in locals():\n",
        "    dt_model = train_decision_tree(\n",
        "        X_train_scaled, X_test_scaled, y_train, y_test,\n",
        "        max_depth=5, min_samples_split=10\n",
        "    )\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  Data not ready. Please run the data preparation cell first.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Model Comparison\n",
        "\n",
        "Performance comparison between Random Forest and Decision Tree models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compare_models(rf_model, dt_model, X_test, y_test):\n",
        "    \"\"\"\n",
        "    Compare two models\n",
        "    \"\"\"\n",
        "    # Predictions\n",
        "    rf_pred = rf_model.predict(X_test)\n",
        "    dt_pred = dt_model.predict(X_test)\n",
        "    \n",
        "    # Accuracy\n",
        "    rf_acc = accuracy_score(y_test, rf_pred)\n",
        "    dt_acc = accuracy_score(y_test, dt_pred)\n",
        "    \n",
        "    print(\"=\" * 60)\n",
        "    print(\"MODEL COMPARISON\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"\\nüìä Test Accuracy:\")\n",
        "    print(f\"   Random Forest: {rf_acc:.4f}\")\n",
        "    print(f\"   Decision Tree: {dt_acc:.4f}\")\n",
        "    print(f\"   Difference: {abs(rf_acc - dt_acc):.4f}\")\n",
        "    \n",
        "    # Confusion matrices\n",
        "    print(f\"\\nüìä Confusion Matrices:\")\n",
        "    print(f\"\\nRandom Forest:\")\n",
        "    print(confusion_matrix(y_test, rf_pred))\n",
        "    print(f\"\\nDecision Tree:\")\n",
        "    print(confusion_matrix(y_test, dt_pred))\n",
        "    \n",
        "    # Comparison DataFrame\n",
        "    comparison = pd.DataFrame({\n",
        "        'Model': ['Random Forest', 'Decision Tree'],\n",
        "        'Accuracy': [rf_acc, dt_acc],\n",
        "        'Difference': [rf_acc - dt_acc, dt_acc - rf_acc]\n",
        "    })\n",
        "    \n",
        "    print(f\"\\nüìä Comparison Table:\")\n",
        "    print(comparison.to_string(index=False))\n",
        "    \n",
        "    return comparison\n",
        "\n",
        "# Compare models (if both are trained)\n",
        "if 'rf_model' in locals() and 'dt_model' in locals():\n",
        "    comparison_df = compare_models(rf_model, dt_model, X_test_scaled, y_test)\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  Both models are not trained. Please train the models first.\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
